# mindx/agents/memory_agent.py
"""
MemoryAgent: The centralized memory and persistence hub for the mindX system.

This agent is responsible for asynchronously saving and retrieving structured,
timestamped memories. The current implementation uses a local filesystem for
immediate robustness and auditability.

The long-term scalability solution for this component is Google's AlloyDB,
which will provide ACID compliance, high availability, and efficient querying
over massive datasets. The direct database connection and ORM logic will be
handled by a dedicated data-access-layer agent, but this agent's structure
is designed for a seamless future transition.
"""
from __future__ import annotations
import asyncio
import aiofiles
import aiofiles.os
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List

# Gracefully handle the ujson dependency. Fallback to standard json if not available.
try:
    import ujson as json_lib
    UJSON_AVAILABLE = True
except ImportError:
    import json as json_lib
    UJSON_AVAILABLE = False

from utils.config import Config, PROJECT_ROOT
from utils.logging_config import get_logger, setup_logging

logger = get_logger(__name__)

class MemoryAgent:
    """Manages the creation, storage, and retrieval of all system memories and logs asynchronously."""

    def __init__(self, config: Optional[Config] = None, log_level: str = "INFO"):
        """
        Initializes the MemoryAgent, configures logging, and ensures its
        hierarchical storage directories exist.
        """
        self.config = config or Config()
        
        # Centralize logging setup
        log_file_enabled = self.config.get("logging.file.enabled", True)
        setup_logging(log_level=log_level, console=True, log_file=log_file_enabled)

        # Define base paths from config with robust defaults
        self.data_path = PROJECT_ROOT / self.config.get("system.data_path", "data")
        self.memory_base_path = self.data_path / "memory"
        self.log_path = self.data_path / "logs" # Used by logging_config
        self.process_trace_path = self.log_path / "process_traces"

        self._initialize_memory_storage()
        if UJSON_AVAILABLE:
            logger.info("ujson library detected, will be used for faster JSON operations.")

    def _initialize_memory_storage(self):
        """
        Ensures all required memory and logging directories exist. This synchronous
        setup action makes the agent self-sufficient on first run.
        """
        try:
            # Create the main log directory (setup_logging might do this, but good to be robust)
            self.log_path.mkdir(parents=True, exist_ok=True)
            self.process_trace_path.mkdir(parents=True, exist_ok=True)
            
            # Create the hierarchical memory structure
            self.memory_base_path.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"MemoryAgent Initialized: All memory and log directories verified/created under '{self.data_path}'.")
        except OSError as e:
            logger.critical(f"FATAL: Failed to create required directory structure under {self.data_path}. Check permissions. Error: {e}", exc_info=True)
            raise

    async def save_memory(self, memory_type: str, category: str, data: Dict[str, Any], metadata: Dict[str, Any]) -> Optional[Path]:
        """
        Saves a memory record to the appropriate hierarchical path (STM or LTM).

        Args:
            memory_type: 'STM' for Short-Term, 'LTM' for Long-Term.
            category: A subfolder to organize memories (e.g., 'llm_interactions', 'bdi_plans').
            data: The core Python dictionary to be saved.
            metadata: A dictionary for extra context (e.g., agent_id, run_id).

        Returns:
            The Path object of the newly created memory file, or None on failure.
        """
        try:
            base_path = self.memory_base_path / memory_type.lower() / category
            base_path.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now()
            filename = f"{timestamp.strftime('%Y%m%d%H%M%S_%f')}.{category.replace('/', '_')}.mem.json"
            filepath = base_path / filename

            memory_record = {
                "timestamp_utc": timestamp.utcnow().isoformat(),
                "memory_type": memory_type,
                "category": category,
                "metadata": metadata,
                "data": data,
            }

            async with aiofiles.open(filepath, "w", encoding="utf-8") as f:
                await f.write(json_lib.dumps(memory_record, indent=2))
            
            logger.debug(f"{memory_type} record saved: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"Failed to save {memory_type} record to {filepath}: {e}", exc_info=True)
            return None

    async def log_process(self, process_name: str, data: Dict[str, Any], metadata: Dict[str, Any]) -> Optional[Path]:
        """
        Logs the state of a specific process to a structured .jsonl file,
        appending a new line for each log entry for efficient, continuous logging.

        Args:
            process_name: Name of the process being logged (e.g., 'mastermind_orchestration').
            data: The structured data representing the state or decision.
            metadata: A dictionary for extra context (e.g., agent_id, run_id, step_name).

        Returns:
            The Path object of the log file, or None on failure.
        """
        filepath = None  # Define filepath here to be accessible in the except block
        try:
            agent_id = metadata.get("agent_id")
            if not agent_id:
                logger.warning("log_process called without 'agent_id' in metadata. Saving to global trace directory.")
                target_dir = self.process_trace_path
                filename = "global_process_trace.jsonl"
            else:
                agent_workspace = self.get_agent_data_directory(str(agent_id))
                target_dir = agent_workspace
                filename = "process_trace.jsonl"

            target_dir.mkdir(parents=True, exist_ok=True)
            filepath = target_dir / filename

            timestamp = datetime.now()
            log_record = {
                "timestamp_utc": timestamp.utcnow().isoformat(),
                "process_name": process_name,
                "metadata": metadata,
                "process_data": data,
            }

            # Use compact JSON representation for .jsonl format
            log_line = json_lib.dumps(log_record) + "\n"

            async with aiofiles.open(filepath, "a", encoding="utf-8") as f:
                await f.write(log_line)

            logger.debug(f"Process trace appended: {filepath}")
            return filepath
        except Exception as e:
            log_path_str = str(filepath) if filepath else "an unknown path"
            logger.error(f"Failed to log process trace to {log_path_str}: {e}", exc_info=True)
            return None

    async def log_terminal_output(self, output: str) -> Optional[Path]:
        """
        Logs raw terminal output to a general, rolling log file.

        Args:
            output: The string content of the terminal output.

        Returns:
            The Path object of the log file, or None on failure.
        """
        try:
            log_file = self.log_path / "mindx_terminal.log"
            async with aiofiles.open(log_file, "a", encoding="utf-8") as f:
                await f.write(f"[{datetime.now().isoformat()}] {output}\n")
            return log_file
        except Exception as e:
            logger.error(f"Failed to write to general terminal log: {e}", exc_info=True)
            return None

    async def save_timestampmemory(
        self,
        agent_id: str,
        input_content: str,
        response_content: str,
        context: Optional[Dict[str, Any]] = None,
        success: bool = True
    ) -> Optional[Path]:
        """
        Save an input-response pair as a timestamped memory record.
        
        Args:
            agent_id: The agent that generated this interaction
            input_content: The input text or data
            response_content: The response text or data
            context: Additional context information
            success: Whether the interaction was successful
            
        Returns:
            The Path object of the saved memory file, or None on failure.
        """
        try:
            timestamp = datetime.now()
            
            # Create timestamped memory directory structure
            agent_memory_dir = self.memory_base_path / "timestamped" / agent_id
            date_dir = agent_memory_dir / timestamp.strftime("%Y%m%d")
            date_dir.mkdir(parents=True, exist_ok=True)
            
            # Create memory record
            memory_record = {
                "timestamp_utc": timestamp.utcnow().isoformat(),
                "timestamp_local": timestamp.isoformat(),
                "agent_id": agent_id,
                "memory_type": "interaction",
                "input": input_content,
                "response": response_content,
                "success": success,
                "context": context or {},
                "metadata": {
                    "memory_version": "1.0",
                    "created_by": "MemoryAgent.save_timestampmemory"
                }
            }
            
            # Generate filename with timestamp
            filename = f"{timestamp.strftime('%Y%m%d%H%M%S_%f')}.timestampmemory.json"
            filepath = date_dir / filename
            
            # Save to file
            async with aiofiles.open(filepath, "w", encoding="utf-8") as f:
                await f.write(json_lib.dumps(memory_record, indent=2))
            
            logger.debug(f"Timestamped memory saved: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Failed to save timestamped memory: {e}", exc_info=True)
            return None

    async def get_recent_timestampmemories(
        self,
        agent_id: str,
        limit: int = 50,
        days_back: int = 7
    ) -> List[Dict[str, Any]]:
        """
        Retrieve recent timestamped memories for an agent.
        
        Args:
            agent_id: The agent to retrieve memories for
            limit: Maximum number of memories to return
            days_back: Number of days to look back
            
        Returns:
            List of memory records
        """
        try:
            memories = []
            agent_memory_dir = self.memory_base_path / "timestamped" / agent_id
            
            if not agent_memory_dir.exists():
                return []
            
            # Get files from the last N days
            for day_offset in range(days_back):
                date_str = (datetime.now() - timedelta(days=day_offset)).strftime("%Y%m%d")
                day_dir = agent_memory_dir / date_str
                
                if not day_dir.exists():
                    continue
                
                # Get all timestampmemory files for this day
                files = sorted(day_dir.glob("*.timestampmemory.json"), reverse=True)
                
                for file_path in files:
                    if len(memories) >= limit:
                        break
                    
                    try:
                        async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                            content = await f.read()
                            memory_record = json_lib.loads(content)
                            memories.append(memory_record)
                    except Exception as e:
                        logger.warning(f"Failed to load memory file {file_path}: {e}")
                        continue
                
                if len(memories) >= limit:
                    break
            
            return memories[:limit]
            
        except Exception as e:
            logger.error(f"Failed to retrieve timestamped memories for {agent_id}: {e}", exc_info=True)
            return []

    async def analyze_agent_memory_patterns(self, agent_id: str, days_back: int = 7) -> Dict[str, Any]:
        """
        Analyze memory patterns for self-awareness.
        
        Args:
            agent_id: The agent to analyze
            days_back: Number of days to analyze
            
        Returns:
            Analysis results with patterns and insights
        """
        try:
            memories = await self.get_recent_timestampmemories(agent_id, limit=1000, days_back=days_back)
            
            if not memories:
                return {"error": f"No memories found for agent {agent_id}"}
            
            # Analyze patterns
            analysis = {
                "agent_id": agent_id,
                "total_memories": len(memories),
                "time_range_days": days_back,
                "success_rate": 0.0,
                "activity_by_hour": {},
                "daily_activity": {},
                "common_contexts": {},
                "error_patterns": [],
                "insights": []
            }
            
            successful_interactions = 0
            hourly_counts = {}
            daily_counts = {}
            context_patterns = {}
            
            for memory in memories:
                # Count successes
                if memory.get("success", True):
                    successful_interactions += 1
                else:
                    analysis["error_patterns"].append({
                        "timestamp": memory.get("timestamp_local"),
                        "input": memory.get("input", "")[:100],  # Truncate for privacy
                        "context": memory.get("context", {})
                    })
                
                # Activity by hour
                try:
                    timestamp = datetime.fromisoformat(memory.get("timestamp_local", ""))
                    hour = timestamp.hour
                    daily_key = timestamp.strftime("%Y-%m-%d")
                    
                    hourly_counts[hour] = hourly_counts.get(hour, 0) + 1
                    daily_counts[daily_key] = daily_counts.get(daily_key, 0) + 1
                except:
                    pass
                
                # Context patterns
                context = memory.get("context", {})
                for key, value in context.items():
                    if isinstance(value, (str, int, float, bool)):
                        pattern_key = f"{key}:{value}"
                        context_patterns[pattern_key] = context_patterns.get(pattern_key, 0) + 1
            
            # Calculate metrics
            analysis["success_rate"] = successful_interactions / len(memories) if memories else 0
            analysis["activity_by_hour"] = hourly_counts
            analysis["daily_activity"] = daily_counts
            
            # Get most common context patterns
            sorted_contexts = sorted(context_patterns.items(), key=lambda x: x[1], reverse=True)
            analysis["common_contexts"] = dict(sorted_contexts[:10])
            
            # Generate insights
            insights = []
            if analysis["success_rate"] < 0.8:
                insights.append(f"Low success rate: {analysis['success_rate']:.1%}")
            if len(analysis["error_patterns"]) > len(memories) * 0.1:
                insights.append(f"High error rate: {len(analysis['error_patterns'])} errors out of {len(memories)} interactions")
            if hourly_counts:
                peak_hour = max(hourly_counts.keys(), key=lambda x: hourly_counts[x])
                insights.append(f"Peak activity hour: {peak_hour}:00 ({hourly_counts[peak_hour]} interactions)")
            
            analysis["insights"] = insights
            
            return analysis
            
        except Exception as e:
            logger.error(f"Failed to analyze memory patterns for {agent_id}: {e}", exc_info=True)
            return {"error": str(e)}

    async def generate_memory_summary(self, agent_id: str, days_back: int = 1) -> str:
        """
        Generate a human-readable memory summary for an agent.
        
        Args:
            agent_id: The agent to summarize
            days_back: Number of days to include in summary
            
        Returns:
            Human-readable summary string
        """
        try:
            analysis = await self.analyze_agent_memory_patterns(agent_id, days_back)
            
            if "error" in analysis:
                return f"Error generating summary for {agent_id}: {analysis['error']}"
            
            summary_lines = [
                f"# Memory Summary for {agent_id}",
                f"Time Period: Last {days_back} day(s)",
                f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                "",
                "## Activity Overview",
                f"- Total interactions: {analysis['total_memories']}",
                f"- Success rate: {analysis['success_rate']:.1%}",
                f"- Error count: {len(analysis['error_patterns'])}",
                ""
            ]
            
            # Daily activity
            if analysis["daily_activity"]:
                summary_lines.extend([
                    "## Daily Activity",
                    *[f"- {date}: {count} interactions" for date, count in analysis["daily_activity"].items()],
                    ""
                ])
            
            # Peak hours
            if analysis["activity_by_hour"]:
                sorted_hours = sorted(analysis["activity_by_hour"].items(), key=lambda x: x[1], reverse=True)
                top_hours = sorted_hours[:3]
                summary_lines.extend([
                    "## Peak Activity Hours",
                    *[f"- {hour}:00 - {count} interactions" for hour, count in top_hours],
                    ""
                ])
            
            # Common contexts
            if analysis["common_contexts"]:
                summary_lines.extend([
                    "## Common Context Patterns",
                    *[f"- {pattern}: {count} times" for pattern, count in list(analysis["common_contexts"].items())[:5]],
                    ""
                ])
            
            # Insights
            if analysis["insights"]:
                summary_lines.extend([
                    "## Key Insights",
                    *[f"- {insight}" for insight in analysis["insights"]],
                    ""
                ])
            
            # Recent errors (if any)
            if analysis["error_patterns"]:
                summary_lines.extend([
                    "## Recent Errors",
                    *[f"- {error['timestamp']}: {error['input']}" for error in analysis["error_patterns"][-3:]],
                    ""
                ])
            
            return "\n".join(summary_lines)
            
        except Exception as e:
            logger.error(f"Failed to generate memory summary: {e}", exc_info=True)
            return f"Error generating summary: {e}"

    def get_agent_data_directory(self, agent_id: str, ensure_exists: bool = True) -> Path:
        """
        Provides a dedicated data directory for a specific agent.

        Args:
            agent_id: The unique identifier for the agent.
            ensure_exists: If True, creates the directory if it doesn't exist.

        Returns:
            The Path object for the agent's data directory.
        """
        # Sanitize the agent_id to create a safe directory name
        safe_agent_id = re.sub(r'[^\w\-\.]', '_', agent_id)
        agent_dir = self.memory_base_path / "agent_workspaces" / safe_agent_id
        
        if ensure_exists:
            try:
                agent_dir.mkdir(parents=True, exist_ok=True)
                # Also ensure the process_traces subdirectory exists for older agents
                (agent_dir / "process_traces").mkdir(exist_ok=True)
                logger.debug(f"Ensured data directory exists for agent '{agent_id}' at: {agent_dir}")
            except OSError as e:
                logger.error(f"Failed to create data directory for agent '{agent_id}': {e}", exc_info=True)
                # Depending on desired robustness, you might want to raise the exception
                # or return a default/fallback path. For now, we log and proceed.
        return agent_dir

    async def retrieve_latest_memories(self, source_type_filter: str = "*", limit: int = 1) -> List[Dict[str, Any]]:
        """
        Retrieves the latest memories.

        Currently retrieves from the local filesystem. In a future build, this
        will be augmented to query AlloyDB for more complex or historical retrievals,
        while using the filesystem for rapid access to very recent events.

        Args:
            source_type_filter: A glob pattern for the source type (e.g., 'action_cycle', '*').
            limit: The maximum number of memories to return.

        Returns:
            A list of memory records (as dictionaries), or an empty list on failure.
        """
        try:
            # The glob pattern correctly matches the 'timestamp.source_type.json' convention.
            files = sorted(self.memory_base_path.glob(f"*.{source_type_filter}.json"), reverse=True)
            
            memories = []
            for file_path in files[:limit]:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    memories.append(json_lib.loads(content))
            return memories
        except Exception as e:
            logger.error(f"Failed to retrieve memories from filesystem with filter '{source_type_filter}': {e}")
            return []

    async def get_runtime_logs(self, limit: int = 100) -> List[str]:
        """
        Retrieves the latest lines from the runtime log file.

        Args:
            limit: The maximum number of log lines to return.

        Returns:
            A list of log lines, or an empty list on failure.
        """
        try:
            log_file = self.log_path / "mindx_runtime.log"
            if not await aiofiles.os.path.exists(log_file):
                return ["Runtime log file not found."]
            
            async with aiofiles.open(log_file, 'r', encoding='utf-8') as f:
                lines = await f.readlines()

            if not lines:
                return ["Runtime log is empty."]
            
            return lines[-limit:]
        except Exception as e:
            logger.error(f"Failed to retrieve runtime logs: {e}")
            return [f"Error retrieving logs: {e}"]
