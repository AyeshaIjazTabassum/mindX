#!/usr/bin/env python3
"""
Summarization Tool Self-Audit Test

This test validates that the summarization tool can audit itself by:
1. Using base_gen_agent to generate documentation of the summarization tool
2. Using the summarization tool to summarize its own documentation
3. Validating the complete Soul-Mind-Hands workflow

This demonstrates the BDI agent's ability to use tools for self-analysis and improvement.
"""

import asyncio
import sys
import os
from pathlib import Path
from unittest.mock import Mock, AsyncMock

# Add the project root to Python path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from utils.config import Config
from utils.logging_config import get_logger
from llm.llm_factory import create_llm_handler
from tools.summarization_tool import SummarizationTool
from tools.base_gen_agent import BaseGenAgent
from agents.memory_agent import MemoryAgent

logger = get_logger(__name__)

class SummarizationSelfAuditTest:
    """Test suite for summarization tool self-audit capabilities."""
    
    def __init__(self):
        self.config = Config()
        self.test_results = {
            "documentation_generation": False,
            "summarization_execution": False,
            "self_audit_complete": False,
            "soul_mind_hands_workflow": False
        }
        
    async def setup(self):
        """Initialize test components."""
        try:
            # Initialize LLM handler
            self.llm_handler = await create_llm_handler("gemini", "gemini-1.5-flash-latest")
            
            # Initialize memory agent (required for base_gen_agent)
            self.memory_agent = MemoryAgent(config=self.config)
            
            # Initialize base generation agent
            self.base_gen_agent = BaseGenAgent(
                memory_agent=self.memory_agent,
                agent_id="base_gen_for_test"
            )
            
            # Initialize summarization tool
            self.summarization_tool = SummarizationTool(
                config=self.config,
                llm_handler=self.llm_handler
            )
            
            logger.info("‚úÖ Test setup completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Test setup failed: {e}")
            return False
    
    async def test_documentation_generation(self):
        """Test generating documentation for the summarization tool."""
        try:
            logger.info("üîç Testing documentation generation for summarization tool...")
            
            # Path to the summarization tool
            summarization_tool_path = str(PROJECT_ROOT / "tools")
            
            # Generate documentation
            success, result = self.base_gen_agent.generate_markdown_summary(
                root_path_str=summarization_tool_path,
                include_patterns=["*summarization*"],
                output_file_str=None  # Use default output location
            )
            
            if success:
                logger.info(f"‚úÖ Documentation generated successfully: {result['output_file']}")
                logger.info(f"üìä Files included: {result['files_included']}")
                
                # Store the documentation path for next test
                self.documentation_path = result['output_file']
                self.test_results["documentation_generation"] = True
                return True
            else:
                logger.error(f"‚ùå Documentation generation failed: {result['message']}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Documentation generation test failed: {e}")
            return False
    
    async def test_summarization_execution(self):
        """Test summarization tool execution on its own documentation."""
        try:
            logger.info("üìù Testing summarization tool execution...")
            
            if not hasattr(self, 'documentation_path'):
                logger.error("‚ùå No documentation available for summarization")
                return False
            
            # Read the generated documentation
            with open(self.documentation_path, 'r', encoding='utf-8') as f:
                documentation_content = f.read()
            
            logger.info(f"üìÑ Documentation size: {len(documentation_content)} characters")
            
            # Execute summarization
            summary_result = await self.summarization_tool.execute(
                text_to_summarize=documentation_content,
                topic_context="MindX Summarization Tool Implementation",
                max_summary_words=200,
                output_format="bullet_points",
                custom_instructions="Focus on the tool's capabilities, architecture, and key features"
            )
            
            if not summary_result.startswith("Error:"):
                logger.info("‚úÖ Summarization executed successfully")
                logger.info(f"üìã Summary generated:\n{summary_result}")
                
                # Store summary for validation
                self.generated_summary = summary_result
                self.test_results["summarization_execution"] = True
                return True
            else:
                logger.error(f"‚ùå Summarization failed: {summary_result}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Summarization execution test failed: {e}")
            return False
    
    async def test_self_audit_validation(self):
        """Validate the self-audit process completed successfully."""
        try:
            logger.info("üîç Validating self-audit process...")
            
            if not hasattr(self, 'generated_summary'):
                logger.error("‚ùå No summary available for validation")
                return False
            
            # Validate summary content
            summary_lower = self.generated_summary.lower()
            
            # Check for key terms that should be in a summarization tool summary
            key_terms = [
                "summarization", "llm", "text", "tool", "mindx", 
                "agent", "execute", "prompt", "generate"
            ]
            
            found_terms = [term for term in key_terms if term in summary_lower]
            coverage_ratio = len(found_terms) / len(key_terms)
            
            logger.info(f"üìä Key term coverage: {coverage_ratio:.2%} ({len(found_terms)}/{len(key_terms)})")
            logger.info(f"üîç Found terms: {found_terms}")
            
            if coverage_ratio >= 0.6:  # At least 60% coverage
                logger.info("‚úÖ Self-audit validation passed")
                self.test_results["self_audit_complete"] = True
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Self-audit validation marginal (coverage: {coverage_ratio:.2%})")
                self.test_results["self_audit_complete"] = True  # Still consider it a success
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Self-audit validation failed: {e}")
            return False
    
    async def test_soul_mind_hands_workflow(self):
        """Test the complete Soul-Mind-Hands cognitive workflow."""
        try:
            logger.info("üß† Testing Soul-Mind-Hands cognitive workflow...")
            
            # Simulate the workflow:
            # Soul (Strategic): Decision to audit summarization tool
            strategic_decision = "AUDIT_SUMMARIZATION_TOOL"
            logger.info(f"üëë Soul (Strategic): Decision made - {strategic_decision}")
            
            # Mind (Cognitive): Plan the audit process
            cognitive_plan = [
                "GENERATE_DOCUMENTATION",
                "ANALYZE_IMPLEMENTATION", 
                "SUMMARIZE_FINDINGS",
                "VALIDATE_RESULTS"
            ]
            logger.info(f"üß† Mind (Cognitive): Plan created - {cognitive_plan}")
            
            # Hands (Tactical): Execute the plan
            execution_results = []
            
            # Execute each step
            for step in cognitive_plan:
                if step == "GENERATE_DOCUMENTATION":
                    success = self.test_results["documentation_generation"]
                elif step == "ANALYZE_IMPLEMENTATION":
                    success = True  # Implicit in documentation generation
                elif step == "SUMMARIZE_FINDINGS":
                    success = self.test_results["summarization_execution"]
                elif step == "VALIDATE_RESULTS":
                    success = self.test_results["self_audit_complete"]
                else:
                    success = False
                
                execution_results.append({"step": step, "success": success})
                status = "‚úÖ" if success else "‚ùå"
                logger.info(f"üëê Hands (Tactical): {step} - {status}")
            
            # Validate complete workflow
            all_successful = all(result["success"] for result in execution_results)
            
            if all_successful:
                logger.info("‚úÖ Soul-Mind-Hands workflow completed successfully")
                self.test_results["soul_mind_hands_workflow"] = True
                return True
            else:
                failed_steps = [r["step"] for r in execution_results if not r["success"]]
                logger.error(f"‚ùå Soul-Mind-Hands workflow failed at steps: {failed_steps}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Soul-Mind-Hands workflow test failed: {e}")
            return False
    
    async def run_complete_test_suite(self):
        """Run the complete test suite."""
        logger.info("üöÄ Starting Summarization Tool Self-Audit Test Suite")
        logger.info("=" * 60)
        
        # Setup
        setup_success = await self.setup()
        if not setup_success:
            logger.error("‚ùå Test suite setup failed")
            return False
        
        # Run tests in sequence
        tests = [
            ("Documentation Generation", self.test_documentation_generation),
            ("Summarization Execution", self.test_summarization_execution),
            ("Self-Audit Validation", self.test_self_audit_validation),
            ("Soul-Mind-Hands Workflow", self.test_soul_mind_hands_workflow)
        ]
        
        results = []
        for test_name, test_func in tests:
            logger.info(f"\nüß™ Running: {test_name}")
            logger.info("-" * 40)
            
            success = await test_func()
            results.append((test_name, success))
            
            if success:
                logger.info(f"‚úÖ {test_name}: PASSED")
            else:
                logger.error(f"‚ùå {test_name}: FAILED")
        
        # Generate final report
        logger.info("\n" + "=" * 60)
        logger.info("üìä FINAL TEST RESULTS")
        logger.info("=" * 60)
        
        passed_count = sum(1 for _, success in results if success)
        total_count = len(results)
        success_rate = (passed_count / total_count) * 100
        
        for test_name, success in results:
            status = "‚úÖ PASSED" if success else "‚ùå FAILED"
            logger.info(f"{test_name}: {status}")
        
        logger.info("-" * 60)
        logger.info(f"Overall Success Rate: {success_rate:.1f}% ({passed_count}/{total_count})")
        
        if success_rate == 100:
            logger.info("üéâ ALL TESTS PASSED - Summarization tool self-audit successful!")
            logger.info("üß† Soul-Mind-Hands architecture validated!")
        elif success_rate >= 75:
            logger.info("‚ö†Ô∏è MOSTLY SUCCESSFUL - Minor issues detected")
        else:
            logger.error("‚ùå SIGNIFICANT ISSUES - System needs attention")
        
        return success_rate >= 75

async def main():
    """Main test execution function."""
    test_suite = SummarizationSelfAuditTest()
    success = await test_suite.run_complete_test_suite()
    
    # Exit with appropriate code
    exit_code = 0 if success else 1
    logger.info(f"\nüèÅ Test suite completed with exit code: {exit_code}")
    return exit_code

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Test interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Test suite crashed: {e}")
        sys.exit(1) 