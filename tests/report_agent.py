# tests/report_agent.py
"""
Report Agent for MindX Testing Framework

This agent specializes in generating comprehensive markdown reports for test results,
system analysis, and cognitive assessments. It can be called by test_agent and other
agents to produce detailed documentation in the style of existing mindX reports.
"""

import asyncio
import json
import time
import uuid
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

# Core mindX imports
from utils.config import Config, PROJECT_ROOT
from utils.logging_config import get_logger
from agents.memory_agent import MemoryAgent

logger = get_logger(__name__)

class ReportType(Enum):
    """Types of reports that can be generated."""
    COGNITION_TEST = "cognition_test"
    SYSTEM_ANALYSIS = "system_analysis"
    PERFORMANCE_ASSESSMENT = "performance_assessment"
    INTEGRATION_TEST = "integration_test"
    SECURITY_AUDIT = "security_audit"

class ReportFormat(Enum):
    """Report formatting styles."""
    TECHNICAL = "technical"
    EXECUTIVE = "executive"
    DETAILED = "detailed"
    SUMMARY = "summary"

@dataclass
class ReportMetadata:
    """Metadata for report generation."""
    report_id: str
    report_type: ReportType
    format_style: ReportFormat
    title: str
    subtitle: Optional[str] = None
    author: str = "MindX Report Agent"
    generated_by: str = "report_agent"
    timestamp: float = None
    version: str = "1.0"
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

class ReportAgent:
    """
    Specialized agent for generating comprehensive markdown reports.
    """
    
    def __init__(self, agent_id: str = "report_agent", output_dir: Optional[Path] = None):
        self.agent_id = agent_id
        self.config = Config()
        self.logger = get_logger(f"ReportAgent.{agent_id}")
        
        # Set output directory
        self.output_dir = output_dir or (PROJECT_ROOT / "tests" / "reports")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self.memory_agent = MemoryAgent(config=self.config)
        
        # Report templates
        self.templates = self._load_report_templates()
        self.report_history: List[str] = []
        
        self.logger.info(f"Report Agent '{self.agent_id}' initialized. Output: {self.output_dir}")
    
    def _load_report_templates(self) -> Dict[str, str]:
        """Load report templates for different types."""
        return {
            "header": """# {title}
{subtitle}

## Executive Summary

{executive_summary}

## Report Details

**Report ID**: `{report_id}`  
**Generated**: {timestamp}  
**Author**: {author}  
**Version**: {version}  
**Type**: {report_type}

---
""",
            
            "test_results_summary": """## Test Results Summary

### ‚úÖ Overall Performance

| Metric | Value | Status |
|--------|-------|--------|
| **Total Tests** | {total_tests} | {test_status} |
| **Success Rate** | {success_rate:.1%} | {success_status} |
| **Execution Time** | {execution_time:.2f}s | {time_status} |
| **System Status** | {system_status} | {system_status_icon} |

### üìä Category Breakdown

{category_breakdown}

### üéØ Key Findings

{key_findings}
""",
            
            "footer": """
---

## Appendix

### A. Technical Details

{technical_details}

### B. Raw Data

{raw_data}

### C. Recommendations

{recommendations}

---

*Report generated by MindX Report Agent v{version} on {timestamp}*
*Session ID: `{session_id}`*
"""
        }
    
    async def generate_report(self, 
                            report_type: ReportType,
                            data: Dict[str, Any],
                            metadata: Optional[ReportMetadata] = None,
                            format_style: ReportFormat = ReportFormat.DETAILED) -> Tuple[bool, str]:
        """Generate a comprehensive markdown report."""
        try:
            # Generate metadata if not provided
            if metadata is None:
                metadata = ReportMetadata(
                    report_id=f"{report_type.value}_{int(time.time())}",
                    report_type=report_type,
                    format_style=format_style,
                    title=self._generate_title(report_type, data),
                    generated_by=self.agent_id
                )
            
            # Generate report content
            report_content = await self._generate_report_content(report_type, data, metadata, format_style)
            
            # Save report to file
            report_filename = f"{metadata.report_id}.md"
            report_path = self.output_dir / report_filename
            
            with report_path.open("w", encoding="utf-8") as f:
                f.write(report_content)
            
            # Log the report generation
            await self._log_report_generation(metadata, str(report_path))
            
            # Add to history
            self.report_history.append(str(report_path))
            
            self.logger.info(f"Report generated successfully: {report_path}")
            return True, str(report_path)
            
        except Exception as e:
            self.logger.error(f"Failed to generate report: {e}", exc_info=True)
            return False, f"Report generation failed: {e}"
    
    async def generate_cognition_test_report(self, test_results: Dict[str, Any]) -> Tuple[bool, str]:
        """Generate a cognition test report from test_agent results."""
        return await self.generate_report(
            ReportType.COGNITION_TEST,
            test_results,
            ReportMetadata(
                report_id=f"cognition_test_{test_results.get('session_id', int(time.time()))}",
                report_type=ReportType.COGNITION_TEST,
                format_style=ReportFormat.DETAILED,
                title="Ultimate Cognition Test Report",
                subtitle="Comprehensive Analysis of MindX Cognitive Capabilities"
            )
        )
    
    async def generate_system_analysis_report(self, analysis_data: Dict[str, Any]) -> Tuple[bool, str]:
        """Generate a system analysis report."""
        return await self.generate_report(
            ReportType.SYSTEM_ANALYSIS,
            analysis_data,
            ReportMetadata(
                report_id=f"system_analysis_{int(time.time())}",
                report_type=ReportType.SYSTEM_ANALYSIS,
                format_style=ReportFormat.TECHNICAL,
                title="MindX System Analysis Report",
                subtitle="Comprehensive System Performance and Health Assessment"
            )
        )
    
    async def _generate_report_content(self, 
                                     report_type: ReportType,
                                     data: Dict[str, Any],
                                     metadata: ReportMetadata,
                                     format_style: ReportFormat) -> str:
        """Generate the complete report content."""
        sections = []
        
        # Header section
        header = self._format_header(metadata, data)
        sections.append(header)
        
        # Content sections based on report type
        if report_type == ReportType.COGNITION_TEST:
            sections.extend(await self._generate_cognition_test_sections(data, format_style))
        elif report_type == ReportType.SYSTEM_ANALYSIS:
            sections.extend(await self._generate_system_analysis_sections(data, format_style))
        else:
            sections.extend(await self._generate_generic_sections(data, format_style))
        
        # Footer section
        footer = self._format_footer(metadata, data)
        sections.append(footer)
        
        return "\n\n".join(sections)
    
    async def _generate_cognition_test_sections(self, data: Dict[str, Any], 
                                              format_style: ReportFormat) -> List[str]:
        """Generate sections specific to cognition test reports."""
        sections = []
        
        # Test Results Summary
        if "execution_summary" in data:
            summary = data["execution_summary"]
            category_breakdown = self._format_category_breakdown(data.get("category_results", []))
            key_findings = self._extract_key_findings(data)
            
            test_results_section = self.templates["test_results_summary"].format(
                total_tests=summary.get("total_tests", 0),
                test_status="‚úÖ PASSED" if summary.get("success_rate", 0) >= 0.8 else "‚ùå FAILED",
                success_rate=summary.get("success_rate", 0),
                success_status="‚úÖ EXCELLENT" if summary.get("success_rate", 0) >= 0.9 else 
                             "‚úÖ GOOD" if summary.get("success_rate", 0) >= 0.8 else "‚ö†Ô∏è NEEDS IMPROVEMENT",
                execution_time=summary.get("total_time", 0),
                time_status="‚úÖ FAST" if summary.get("total_time", 0) < 30 else "‚ö†Ô∏è SLOW",
                system_status=summary.get("system_status", "UNKNOWN"),
                system_status_icon=self._get_status_icon(summary.get("system_status", "UNKNOWN")),
                category_breakdown=category_breakdown,
                key_findings=key_findings
            )
            sections.append(test_results_section)
        
        # Detailed Test Results
        if "test_results" in data:
            detailed_section = self._format_detailed_test_results(data["test_results"])
            sections.append(detailed_section)
        
        return sections
    
    async def _generate_system_analysis_sections(self, data: Dict[str, Any], 
                                               format_style: ReportFormat) -> List[str]:
        """Generate sections for system analysis reports."""
        sections = []
        
        # System Health Assessment
        health_section = f"""## System Health Assessment

### ÔøΩÔøΩ Component Status

{self._format_component_status(data.get("components", {}))}

### üìà Performance Trends

{self._format_performance_trends(data.get("performance", {}))}

### ‚ö†Ô∏è Issues & Recommendations

{self._format_issues_and_recommendations(data.get("issues", []))}
"""
        sections.append(health_section)
        
        return sections
    
    async def _generate_generic_sections(self, data: Dict[str, Any], 
                                       format_style: ReportFormat) -> List[str]:
        """Generate generic sections for unknown report types."""
        sections = []
        
        generic_section = f"""## Report Data

### üìã Summary

{self._format_generic_summary(data)}

### üìä Data Analysis

{self._format_generic_data_analysis(data)}
"""
        sections.append(generic_section)
        
        return sections
    
    def _format_header(self, metadata: ReportMetadata, data: Dict[str, Any]) -> str:
        """Format the report header."""
        executive_summary = self._generate_executive_summary(metadata.report_type, data)
        
        return self.templates["header"].format(
            title=metadata.title,
            subtitle=f"*{metadata.subtitle}*" if metadata.subtitle else "",
            executive_summary=executive_summary,
            report_id=metadata.report_id,
            timestamp=datetime.fromtimestamp(metadata.timestamp).strftime("%Y-%m-%d %H:%M:%S UTC"),
            author=metadata.author,
            version=metadata.version,
            report_type=metadata.report_type.value.replace("_", " ").title()
        )
    
    def _format_footer(self, metadata: ReportMetadata, data: Dict[str, Any]) -> str:
        """Format the report footer."""
        technical_details = self._format_technical_details(data)
        raw_data = self._format_raw_data(data)
        recommendations = self._format_recommendations(data)
        
        return self.templates["footer"].format(
            technical_details=technical_details,
            raw_data=raw_data,
            recommendations=recommendations,
            version=metadata.version,
            timestamp=datetime.fromtimestamp(metadata.timestamp).strftime("%Y-%m-%d %H:%M:%S"),
            session_id=data.get("session_id", metadata.report_id)
        )
    
    def _generate_title(self, report_type: ReportType, data: Dict[str, Any]) -> str:
        """Generate an appropriate title for the report type."""
        titles = {
            ReportType.COGNITION_TEST: "Ultimate Cognition Test Report",
            ReportType.SYSTEM_ANALYSIS: "MindX System Analysis Report",
            ReportType.PERFORMANCE_ASSESSMENT: "Performance Assessment Report",
            ReportType.INTEGRATION_TEST: "Integration Test Report",
            ReportType.SECURITY_AUDIT: "Security Audit Report"
        }
        return titles.get(report_type, "MindX Report")
    
    def _generate_executive_summary(self, report_type: ReportType, data: Dict[str, Any]) -> str:
        """Generate an executive summary based on report type and data."""
        if report_type == ReportType.COGNITION_TEST:
            summary = data.get("execution_summary", {})
            success_rate = summary.get("success_rate", 0)
            system_status = summary.get("system_status", "UNKNOWN")
            
            return f"""This report presents the results of comprehensive cognitive testing performed on the MindX Augmentic Intelligence system. The testing evaluated {summary.get("total_tests", 0)} distinct cognitive capabilities across multiple domains including reasoning, memory integration, belief consistency, tool coordination, and failure recovery.

**Key Results**: The system achieved a {success_rate:.1%} success rate with an overall status of **{system_status}**. Testing completed in {summary.get("total_time", 0):.1f} seconds, demonstrating {self._get_performance_assessment(success_rate)} cognitive performance across all evaluated domains."""
        
        else:
            return f"""This report provides analysis and assessment results for the MindX system. The report contains detailed findings, metrics, and recommendations based on the {report_type.value.replace('_', ' ')} evaluation."""
    
    def _format_category_breakdown(self, category_results: List[Dict[str, Any]]) -> str:
        """Format category breakdown as a table."""
        if not category_results:
            return "*No category data available*"
        
        table = "| Category | Tests | Success Rate | Time (s) | Status |\n"
        table += "|----------|-------|-------------|----------|--------|\n"
        
        for category in category_results:
            status_icon = "‚úÖ" if category.get("success_rate", 0) >= 0.8 else "‚ö†Ô∏è" if category.get("success_rate", 0) >= 0.6 else "‚ùå"
            table += f"| **{category.get('category', 'Unknown')}** | {category.get('tests_run', 0)} | {category.get('success_rate', 0):.1%} | {category.get('execution_time', 0):.1f} | {status_icon} |\n"
        
        return table
    
    def _extract_key_findings(self, data: Dict[str, Any]) -> str:
        """Extract and format key findings from test data."""
        findings = []
        
        # Analyze success rates
        summary = data.get("execution_summary", {})
        success_rate = summary.get("success_rate", 0)
        
        if success_rate >= 0.9:
            findings.append("üéØ **Excellent Performance**: System demonstrates exceptional cognitive capabilities")
        elif success_rate >= 0.8:
            findings.append("‚úÖ **Good Performance**: System shows solid cognitive functionality")
        elif success_rate >= 0.6:
            findings.append("‚ö†Ô∏è **Acceptable Performance**: System meets basic cognitive requirements")
        else:
            findings.append("‚ùå **Performance Issues**: System requires significant cognitive improvements")
        
        # Analyze recommendations
        recommendations = data.get("recommendations", [])
        if recommendations:
            findings.append(f"üí° **Recommendations**: {len(recommendations)} improvement suggestions identified")
        
        return "\n".join(f"- {finding}" for finding in findings)
    
    def _format_detailed_test_results(self, test_results: List[Dict[str, Any]]) -> str:
        """Format detailed test results section."""
        if not test_results:
            return "## Detailed Test Results\n\n*No detailed test results available*"
        
        section = "## Detailed Test Results\n\n"
        
        for i, test in enumerate(test_results, 1):
            status_icon = "‚úÖ" if test.get("success", False) else "‚ùå"
            section += f"### {i}. {test.get('test_name', 'Unknown Test')} {status_icon}\n\n"
            section += f"**Execution Time**: {test.get('execution_time', 0):.3f}s  \n"
            section += f"**Cognitive Depth**: {test.get('cognitive_depth', 0)}  \n"
            section += f"**Timestamp**: {test.get('formatted_time', 'Unknown')}  \n\n"
            
            # Add test details
            details = test.get("details", {})
            if details:
                section += "**Details**:\n"
                for key, value in details.items():
                    if isinstance(value, (dict, list)):
                        section += f"- **{key}**: `{json.dumps(value, default=str)[:100]}...`\n"
                    else:
                        section += f"- **{key}**: {value}\n"
            
            section += "\n---\n\n"
        
        return section
    
    def _get_status_icon(self, status: str) -> str:
        """Get appropriate icon for status."""
        status_icons = {
            "EXCELLENT": "üåü",
            "GOOD": "‚úÖ",
            "ACCEPTABLE": "‚ö†Ô∏è",
            "NEEDS_IMPROVEMENT": "üîß",
            "CRITICAL_ISSUES": "‚ùå",
            "UNKNOWN": "‚ùì"
        }
        return status_icons.get(status.upper(), "‚ùì")
    
    def _get_performance_assessment(self, success_rate: float) -> str:
        """Get performance assessment text."""
        if success_rate >= 0.9:
            return "exceptional"
        elif success_rate >= 0.8:
            return "strong"
        elif success_rate >= 0.6:
            return "adequate"
        else:
            return "concerning"
    
    # Placeholder formatting methods
    def _format_component_status(self, components: Dict[str, Any]) -> str:
        """Format component status information."""
        if not components:
            return "*No component status data available*"
        
        status_table = "| Component | Status | Health | Notes |\n"
        status_table += "|-----------|--------|--------|-------|\n"
        
        for comp_name, comp_data in components.items():
            status = comp_data.get("status", "Unknown")
            health = comp_data.get("health", "Unknown")
            notes = comp_data.get("notes", "")
            
            status_icon = "‚úÖ" if status == "Active" else "‚ùå" if status == "Failed" else "‚ö†Ô∏è"
            status_table += f"| {comp_name} | {status} {status_icon} | {health} | {notes} |\n"
        
        return status_table
    
    def _format_performance_trends(self, performance: Dict[str, Any]) -> str:
        """Format performance trends."""
        return "*Performance trend analysis available in full system monitoring*"
    
    def _format_issues_and_recommendations(self, issues: List[str]) -> str:
        """Format issues and recommendations."""
        if not issues:
            return "‚úÖ **No critical issues identified**"
        
        formatted = "**Issues Identified**:\n\n"
        for i, issue in enumerate(issues, 1):
            formatted += f"{i}. {issue}\n"
        
        return formatted
    
    def _format_generic_summary(self, data: Dict[str, Any]) -> str:
        """Format generic data summary."""
        summary = f"**Data Keys**: {list(data.keys())}\n\n"
        
        for key, value in data.items():
            if isinstance(value, dict):
                summary += f"- **{key}**: Dictionary with {len(value)} entries\n"
            elif isinstance(value, list):
                summary += f"- **{key}**: List with {len(value)} items\n"
            else:
                summary += f"- **{key}**: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}\n"
        
        return summary
    
    def _format_generic_data_analysis(self, data: Dict[str, Any]) -> str:
        """Format generic data analysis."""
        return f"```json\n{json.dumps(data, indent=2, default=str)[:1000]}...\n```"
    
    def _format_technical_details(self, data: Dict[str, Any]) -> str:
        """Format technical details section."""
        return f"""
**System Environment**:
- Python Version: 3.10+
- MindX Version: Production Build
- Report Agent Version: 1.0
- Generation Time: {datetime.now().isoformat()}

**Data Summary**:
- Total Data Points: {len(data)}
- Data Size: ~{len(json.dumps(data, default=str))} bytes
"""
    
    def _format_raw_data(self, data: Dict[str, Any]) -> str:
        """Format raw data section (truncated)."""
        raw_json = json.dumps(data, indent=2, default=str)
        if len(raw_json) > 2000:
            return f"```json\n{raw_json[:2000]}...\n```\n\n*Note: Raw data truncated for readability.*"
        else:
            return f"```json\n{raw_json}\n```"
    
    def _format_recommendations(self, data: Dict[str, Any]) -> str:
        """Format recommendations section."""
        recommendations = data.get("recommendations", [])
        
        if not recommendations:
            return "‚úÖ **No specific recommendations at this time**"
        
        formatted = ""
        for i, rec in enumerate(recommendations, 1):
            formatted += f"{i}. {rec}\n"
        
        return formatted
    
    async def _log_report_generation(self, metadata: ReportMetadata, file_path: str):
        """Log report generation to memory system."""
        log_data = {
            "report_id": metadata.report_id,
            "report_type": metadata.report_type.value,
            "file_path": file_path,
            "timestamp": metadata.timestamp,
            "generated_by": metadata.generated_by
        }
        
        await self.memory_agent.log_process(
            "report_generation",
            log_data,
            {"agent_id": self.agent_id}
        )
    
    def get_report_history(self) -> List[str]:
        """Get list of generated report file paths."""
        return self.report_history.copy()

# Convenience functions for easy integration
async def generate_cognition_test_report(test_results: Dict[str, Any], 
                                       output_dir: Optional[Path] = None) -> Tuple[bool, str]:
    """Convenience function to generate cognition test report."""
    report_agent = ReportAgent(output_dir=output_dir)
    return await report_agent.generate_cognition_test_report(test_results)

# Main execution for testing
async def main():
    """Test the report agent functionality."""
    print("üìä MindX Report Agent Test")
    print("=" * 40)
    
    # Create test data
    test_data = {
        "session_id": "test_session_123",
        "execution_summary": {
            "total_tests": 10,
            "successful_tests": 8,
            "success_rate": 0.8,
            "total_time": 25.5,
            "system_status": "GOOD"
        },
        "test_results": [
            {
                "test_name": "belief_system_test",
                "success": True,
                "execution_time": 1.23,
                "cognitive_depth": 2,
                "formatted_time": "2025-01-01T12:00:00",
                "details": {"beliefs_tested": 5, "success_rate": 0.8}
            }
        ],
        "recommendations": [
            "Improve memory integration performance",
            "Optimize cognitive loop timing"
        ]
    }
    
    # Generate test report
    report_agent = ReportAgent()
    success, report_path = await report_agent.generate_cognition_test_report(test_data)
    
    if success:
        print(f"‚úÖ Report generated successfully: {report_path}")
    else:
        print(f"‚ùå Report generation failed: {report_path}")

if __name__ == "__main__":
    asyncio.run(main())
